{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b887c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\LangChain project\\learn-LangChain\\learnlangchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e7fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatModel = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0.2,\n",
    "    max_output_tokens=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b804ca11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/langchain.txt'}, page_content=\"What is LangChain?\\nLangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queriesâ€”for example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain also includes components that allow LLMs to access new data sets without retraining.\\n\\nRead about Large Language Models (LLMs)\\n\\nWhy is LangChain important?\\nLLMs excel at responding to prompts in a general context, but struggle in a specific domain they were never trained on. Prompts are queries people use to seek responses from an LLM. For example, an LLM can provide an answer to how much a computer costs by providing an estimate. However, it can't list the price of a specific computer model that your company sells. \\n\\nTo do that, machine learning engineers must integrate the LLM with the organizationâ€™s internal data sources and apply prompt engineeringâ€”a practice where a data scientist refines inputs to a generative model with a specific structure and context. \\n\\nLangChain streamlines intermediate steps to develop such data-responsive applications, making prompt engineering more efficient. It is designed to develop diverse applications powered by language models more effortlessly, including chatbots, question-answering, content generation, summarizers, and more.\\n\\nThe following sections describe benefits of LangChain.\\n\\nRepurpose language models\\nWith LangChain, organizations can repurpose LLMs for domain-specific applications without retraining or fine-tuning. Development teams can build complex applications referencing proprietary information to augment model responses. For example, you can use LangChain to build applications that read data from stored internal documents and summarize them into conversational responses. You can create a Retrieval Augmented Generation (RAG) workflow that introduces new information to the language model during prompting. Implementing context-aware workflows like RAG reduces model hallucination and improves response accuracy. \\n\\nSimplify AI development\\nLangChain simplifies artificial intelligence (AI) development by abstracting the complexity of data source integrations and prompt refining. Developers can customize sequences to build complex applications quickly. Instead of programming business logic, software teams can modify templates and libraries that LangChain provides to reduce development time. \\n\\nDeveloper support\\nLangChain provides AI developers with tools to connect language models with external data sources. It is open-source and supported by an active community. Organizations can use LangChain for free and receive support from other developers proficient in the framework.\\n\\nHow does LangChain work?\\nWith LangChain, developers can adapt a language model flexibly to specific business contexts by designating steps required to produce the desired outcome. \\n\\nChains\\nChains are the fundamental principle that holds various AI components in LangChain to provide context-aware responses. A chain is a series of automated actions from the user's query to the model's output. For example, developers can use a chain for:\\n\\nConnecting to different data sources.\\nGenerating unique content.\\nTranslating multiple languages.\\nAnswering user queries. \\nLinks\\nChains are made of links. Each action that developers string together to form a chained sequence is called a link. With links, developers can divide complex tasks into multiple, smaller tasks. Examples of links include:\\n\\nFormatting user input. \\nSending a query to an LLM. \\nRetrieving data from cloud storage.\\nTranslating from one language to another.\\nIn the LangChain framework, a link accepts input from the user and passes it to the LangChain libraries for processing. LangChain also allows link reordering to create different AI workflows. \\n\\nWhat are the core components of LangChain?\\nUsing LangChain, software teams can build context-aware language model systems with the following modules. \\n\\nLLM interface\\nLangChain provides APIs with which developers can connect and query LLMs from their code. Developers can interface with public and proprietary models like GPT, Bard, and PaLM with LangChain by making simple API calls instead of writing complex code.\\n\\nPrompt templates\\nPrompt templates are pre-built structures developers use to consistently and precisely format queries for AI models. Developers can create a prompt template for chatbot applications, few-shot learning, or deliver specific instructions to the language models. Moreover, they can reuse the templates across different applications and language models. \\n\\nAgents\\nDevelopers use tools and libraries that LangChain provides to compose and customize existing chains for complex applications. An agent is a special chain that prompts the language model to decide the best sequence in response to a query. When using an agent, developers provide the user's input, available tools, and possible intermediate steps to achieve the desired results. Then, the language model returns a viable sequence of actions the application can take.  \\n\\nRetrieval modules\\nLangChain enables the architecting of RAG systems with numerous tools to transform, store, search, and retrieve information that refine language model responses. Developers can create semantic representations of information with word embeddings and store them in local or cloud vector databases. \\n\\nMemory\\nSome conversational language model applications refine their responses with information recalled from past interactions. LangChain allows developers to include memory capabilities in their systems. It supports:\\n\\nSimple memory systems that recall the most recent conversations. \\nComplex memory structures that analyze historical messages to return the most relevant results. \\nCallbacks\\nCallbacks are codes that developers place in their applications to log, monitor, and stream specific events in LangChain operations. For example, developers can track when a chain was first called and errors encountered with callbacks. \")]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/langchain.txt\")\n",
    "loaded_data = loader.load()\n",
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7298b473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/langchain.txt'}, page_content='What is LangChain?\\nLangChain is an open source framework for building applications based on large language models (LLMs). LLMs are large deep-learning models pre-trained on large amounts of data that can generate responses to user queriesâ€”for example, answering questions or creating images from text-based prompts. LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new prompt chains or customize existing templates. LangChain also includes components that allow LLMs to access new data sets without retraining.\\n\\nRead about Large Language Models (LLMs)'),\n",
       " Document(metadata={'source': 'data/langchain.txt'}, page_content=\"Read about Large Language Models (LLMs)\\n\\nWhy is LangChain important?\\nLLMs excel at responding to prompts in a general context, but struggle in a specific domain they were never trained on. Prompts are queries people use to seek responses from an LLM. For example, an LLM can provide an answer to how much a computer costs by providing an estimate. However, it can't list the price of a specific computer model that your company sells. \\n\\nTo do that, machine learning engineers must integrate the LLM with the organizationâ€™s internal data sources and apply prompt engineeringâ€”a practice where a data scientist refines inputs to a generative model with a specific structure and context. \\n\\nLangChain streamlines intermediate steps to develop such data-responsive applications, making prompt engineering more efficient. It is designed to develop diverse applications powered by language models more effortlessly, including chatbots, question-answering, content generation, summarizers, and more.\"),\n",
       " Document(metadata={'source': 'data/langchain.txt'}, page_content='The following sections describe benefits of LangChain.\\n\\nRepurpose language models\\nWith LangChain, organizations can repurpose LLMs for domain-specific applications without retraining or fine-tuning. Development teams can build complex applications referencing proprietary information to augment model responses. For example, you can use LangChain to build applications that read data from stored internal documents and summarize them into conversational responses. You can create a Retrieval Augmented Generation (RAG) workflow that introduces new information to the language model during prompting. Implementing context-aware workflows like RAG reduces model hallucination and improves response accuracy.'),\n",
       " Document(metadata={'source': 'data/langchain.txt'}, page_content='Simplify AI development\\nLangChain simplifies artificial intelligence (AI) development by abstracting the complexity of data source integrations and prompt refining. Developers can customize sequences to build complex applications quickly. Instead of programming business logic, software teams can modify templates and libraries that LangChain provides to reduce development time. \\n\\nDeveloper support\\nLangChain provides AI developers with tools to connect language models with external data sources. It is open-source and supported by an active community. Organizations can use LangChain for free and receive support from other developers proficient in the framework.\\n\\nHow does LangChain work?\\nWith LangChain, developers can adapt a language model flexibly to specific business contexts by designating steps required to produce the desired outcome.'),\n",
       " Document(metadata={'source': 'data/langchain.txt'}, page_content=\"How does LangChain work?\\nWith LangChain, developers can adapt a language model flexibly to specific business contexts by designating steps required to produce the desired outcome. \\n\\nChains\\nChains are the fundamental principle that holds various AI components in LangChain to provide context-aware responses. A chain is a series of automated actions from the user's query to the model's output. For example, developers can use a chain for:\\n\\nConnecting to different data sources.\\nGenerating unique content.\\nTranslating multiple languages.\\nAnswering user queries. \\nLinks\\nChains are made of links. Each action that developers string together to form a chained sequence is called a link. With links, developers can divide complex tasks into multiple, smaller tasks. Examples of links include:\"),\n",
       " Document(metadata={'source': 'data/langchain.txt'}, page_content='Formatting user input. \\nSending a query to an LLM. \\nRetrieving data from cloud storage.\\nTranslating from one language to another.\\nIn the LangChain framework, a link accepts input from the user and passes it to the LangChain libraries for processing. LangChain also allows link reordering to create different AI workflows. \\n\\nWhat are the core components of LangChain?\\nUsing LangChain, software teams can build context-aware language model systems with the following modules. \\n\\nLLM interface\\nLangChain provides APIs with which developers can connect and query LLMs from their code. Developers can interface with public and proprietary models like GPT, Bard, and PaLM with LangChain by making simple API calls instead of writing complex code.'),\n",
       " Document(metadata={'source': 'data/langchain.txt'}, page_content=\"Prompt templates\\nPrompt templates are pre-built structures developers use to consistently and precisely format queries for AI models. Developers can create a prompt template for chatbot applications, few-shot learning, or deliver specific instructions to the language models. Moreover, they can reuse the templates across different applications and language models. \\n\\nAgents\\nDevelopers use tools and libraries that LangChain provides to compose and customize existing chains for complex applications. An agent is a special chain that prompts the language model to decide the best sequence in response to a query. When using an agent, developers provide the user's input, available tools, and possible intermediate steps to achieve the desired results. Then, the language model returns a viable sequence of actions the application can take.\"),\n",
       " Document(metadata={'source': 'data/langchain.txt'}, page_content='Retrieval modules\\nLangChain enables the architecting of RAG systems with numerous tools to transform, store, search, and retrieve information that refine language model responses. Developers can create semantic representations of information with word embeddings and store them in local or cloud vector databases. \\n\\nMemory\\nSome conversational language model applications refine their responses with information recalled from past interactions. LangChain allows developers to include memory capabilities in their systems. It supports:\\n\\nSimple memory systems that recall the most recent conversations. \\nComplex memory structures that analyze historical messages to return the most relevant results. \\nCallbacks\\nCallbacks are codes that developers place in their applications to log, monitor, and stream specific events in LangChain operations. For example, developers can track when a chain was first called and errors encountered with callbacks.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,  \n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "chunks_of_texts = text_splitter.split_documents(loaded_data)\n",
    "chunks_of_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6162fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "  model=\"models/gemini-embedding-001\", \n",
    "  google_api_key=GEMINI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034504ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks_of_texts,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04c2985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x28ecb44a420>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33a9fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed33ddca",
   "metadata": {},
   "source": [
    "## LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acf55a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0988042",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that answers questions based on the provided context.\"),\n",
    "        (\"human\", \"{context}\\n\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45df6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b9f97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompts\n",
    "    | chatModel\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04284686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided text, here are the advantages of LangChain:\\n\\n*   **Repurposing Language Models:** Allows organizations to adapt LLMs for specific applications without retraining or fine-tuning. It enables the creation of applications that use proprietary information to augment model responses, like summarizing internal documents. This is achieved through Retrieval Augmented Generation (RAG) workflows, which improve response accuracy and reduce model hallucination.\\n*   **Simplifying AI Development:** Abstracts the complexity of data source integrations and prompt refining, allowing developers to customize sequences and build complex applications quickly. It reduces development time by providing templates and libraries.\\n*   **Developer Support:** It is open-source and supported by an active community, providing developers with tools to connect language models with external data sources. Organizations can use it for free and receive support from other developers.\\n*   **Enabling RAG Systems:** Provides tools to transform, store, search, and retrieve information to refine language model responses. Developers can create semantic representations of information and store them in vector databases.\\n*   **Memory Capabilities:** Allows developers to include memory capabilities in their systems, supporting both simple and complex memory structures to refine responses based on past interactions.\\n*   **Callbacks:** Provides callbacks to log, monitor, and stream specific events in LangChain operations, such as tracking when a chain was first called and errors encountered.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"What are the advantages of LangChain?\")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnlangchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
